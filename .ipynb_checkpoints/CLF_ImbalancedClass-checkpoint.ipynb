{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Imbalanced Classes:\n",
    "\n",
    "#### Introduction\n",
    "The inspiration behind this exercise came from a horribly failed exercise during a job interview process. The goal was to accurately predict a binary outcome in which the positive case occured 0.0005% of the time (1/200,000). The dataset consisted of around 6.5M rows and 503 input columns. They gave me a two hour time limit, and I didn't do very well at all. I won't let that happen again.\n",
    "\n",
    "This exercise roughly follows a walkthrough published by EliteDataScience titled \"How to Handle Imbalanced Classes in Machine Learning\" which was written on July 5, 2017 and can be accessed at https://elitedatascience.com/imbalanced-classes. In their walkthrough, they didn't use a test/train split, and they used only accuracy in assessing each model's performance. I did use a test train split, and I explored model performance more thoroughly using scikit learn's classification_report, found in sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "#jtplot.style(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "The data for this exercise comes from the UCI Machine Learning Repository. It is a synthetic dataset that uses four input variables to predict whether a scale is balanced. The output in the original data had three labels: leaning left, balanced, and leaning right. To turn this into a binary classification problem, I simply focused on whether the final result was balanced. This outcome happens only 8% of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   B  1  1.1  1.2  1.3\n",
       "0  R  1    1    1    2\n",
       "1  R  1    1    1    3\n",
       "2  R  1    1    1    4\n",
       "3  R  1    1    1    5\n",
       "4  R  1    1    2    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and glimpse data\n",
    "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    576\n",
       "1     48\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change outcome to binary variable\n",
    "df.columns = ['balance', 'var1', 'var2', 'var3', 'var4']\n",
    "df['balance'] = [1 if b == 'B' else 0 for b in df.balance]\n",
    "df['balance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "# Separate input features (X) and target variable (y)\n",
    "y = df.balance\n",
    "X = df.drop('balance', axis = 1)\n",
    "\n",
    "# Partition into test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 314\n",
    ")\n",
    "\n",
    "# Reconnect x and y for each set to make resampling easier\n",
    "train = X_train.copy()\n",
    "train['balance'] = y_train\n",
    "test = X_test.copy()\n",
    "test['balance'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "clf_lr = LogisticRegression().fit(X_train, y_train)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_lr = clf_lr.predict(X_test)\n",
    "\n",
    "# Observe model performance\n",
    "print('Logistic Regression')\n",
    "print(classification_report(y_test, pred_y_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced Classes: Problems and Solutions\n",
    "Here we see the issue with imbalanced classification. The accuracy for this model is an impressive 92%. Amazing! A novice data scientist may see these results, communicate them to stakeholders and get that model into production. Doing so would be a huge mistake. Further examination of the model shows that it predicts the negative class 100% of the time. A model with such predictions is completely useless, and implementing it would be equivalent to saying \"We should plan on this event never happening.\"\n",
    "\n",
    "But what can we do? Sit around and wait for more data? Wait for that rare event to happen enough that we can get a moderately strong signal out of a massive amount of data? That might be the best option depending on the importance of the model, but chances are you won't want to wait that long. In approaching this problem, we have two paths to a solution. First, we can adjust the data. We can artificially inflate the number of positive cases in the data (upsampling) to strengthen the signal, or we can reduce the number of negative cases (downsampling) to minimize the noise. Upsampling the positive case happens more frequently, as it doesn't result in drastic reduction of the size of the dataset. Upsampling is performed by sampling from the positive cases with replacement until a suffient sample size is collected or by artificially creating additional observations using SMOTE or ADASYN methods. For simplicity's sake, I will stick to sampling with replacement in this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1: Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "train_majority = train[train.balance == 0]\n",
    "train_minority = train[train.balance == 1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample minority class\n",
    "train_minority_upsampled = resample(\n",
    "    train_minority, \n",
    "    replace = True,     # sample with replacement\n",
    "    n_samples = train_majority.shape[0],    # to match majority class\n",
    "    random_state = 123\n",
    ")\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "train_upsampled = pd.concat([train_majority, train_minority_upsampled])\n",
    "\n",
    "# Minor upsample logistic regression\n",
    "y = train_upsampled.balance\n",
    "X = train_upsampled.drop('balance', axis = 1)\n",
    "\n",
    "clf_lr_min = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    27\n",
       "0    27\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Down sample major class\n",
    "train_majority_downsampled = resample(\n",
    "    train_majority,\n",
    "    replace = False,\n",
    "    n_samples = train_minority.shape[0],\n",
    "    random_state = 123\n",
    ")\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "train_downsampled = pd.concat([train_minority, train_majority_downsampled])\n",
    "\n",
    "# Display new class counts\n",
    "train_downsampled.balance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major downsample logistic regression\n",
    "y = train_downsampled.balance\n",
    "X = train_downsampled.drop('balance', axis = 1)\n",
    "\n",
    "clf_lr_maj = LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2: Model Selection\n",
    "I mentioned earlier that there are two approaches to handling imbalanced data. We just walked through the first option, which is to adjust the data on which a model is fitted. The second option is to use a model that better handles imbalanced data. Two model types that often perform well on imbalanced classes are support vector machines (SVM) and tree-based models such as random forests. SVM's can be adjusted to penalize mistakes on the minority class based on how under-represented it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Penalized SVM\n",
    "y = y_train\n",
    "X = X_train\n",
    " \n",
    "clf_sv = SVC(\n",
    "    kernel = 'linear', \n",
    "    class_weight = 'balanced', # penalize\n",
    "    probability = True\n",
    ")\n",
    " \n",
    "clf_sv.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "clf_rf = RandomForestClassifier().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pursuing both types of corrections to adjust for an imbalanced output, we have four potential models to use. We can determine which model fits best by observing its performance on the testing data set aside earlier. Rather than using accuracy, imbalanced class predictors can be more effectively compared using measures such as precision, recall, and the f1-score, which are all computed in scikit learns classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94       167\n",
      "          1       0.00      0.00      0.00        21\n",
      "\n",
      "avg / total       0.79      0.89      0.84       188\n",
      "\n",
      "Logistic Regression Minority Upsample\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.49      0.62       167\n",
      "          1       0.09      0.38      0.14        21\n",
      "\n",
      "avg / total       0.77      0.47      0.57       188\n",
      "\n",
      "Logistic Regression Majority Downsample\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.49      0.62       167\n",
      "          1       0.09      0.43      0.16        21\n",
      "\n",
      "avg / total       0.78      0.48      0.57       188\n",
      "\n",
      "Penalized SVM\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.60      0.72       167\n",
      "          1       0.11      0.38      0.17        21\n",
      "\n",
      "avg / total       0.80      0.58      0.66       188\n",
      "\n",
      "Random Forest\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.98      0.93       167\n",
      "          1       0.00      0.00      0.00        21\n",
      "\n",
      "avg / total       0.79      0.87      0.83       188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bradensharp/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "pred_y_lr = clf_lr.predict(X_test)\n",
    "pred_y_lr_min = clf_lr_min.predict(X_test)\n",
    "pred_y_lr_maj = clf_lr_maj.predict(X_test)\n",
    "pred_y_sv = clf_sv.predict(X_test)\n",
    "pred_y_rf = clf_rf.predict(X_test)\n",
    "\n",
    "print('Logistic Regression')\n",
    "print(classification_report(y_test, pred_y_lr))\n",
    "\n",
    "print('Logistic Regression Minority Upsample')\n",
    "print(classification_report(y_test, pred_y_lr_min))\n",
    "\n",
    "print('Logistic Regression Majority Downsample')\n",
    "print(classification_report(y_test, pred_y_lr_maj))\n",
    "\n",
    "print('Penalized SVM')\n",
    "print(classification_report(y_test, pred_y_sv))\n",
    "\n",
    "print('Random Forest')\n",
    "print(classification_report(y_test, pred_y_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Interestingly enough, performance for the data-adjusted logistic regression models was very similar to that of the penalized SVM. Surprisingly, the random forest, which in many cases would be the top performer by far, failed to correctly classify a single positive case in the testing data. The downsampled model managed to perform as well as the SVM despite being fit on around 60 observations versus the upsampled and SVM's 700. The SVM performed the best, and without consideration for training time and complexity of production, should be implemented. However, since the logistic regression models are simpler, they may be the better choice depending on the real-world context of the model.\n",
    "\n",
    "An interesting note on the random forest: In the original walkthrough by EliteDataScience, the random forest model performed incredibly well, but they didn't use a train/test set to verify that performance. This highlights the dangers of overfitting and the importance of properly validating a model."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten Digit Classification\n",
    "\n",
    "\n",
    "#### Introduction\n",
    "In an increasingly technological world, the digitization of handwritten records is a major undertaking. The benefits of having digital records are immense, and as such, we began the monotonous task of transitioning those records by hand long before computers were intelligent enough to recognize letters and numbers from images of handwritten letters.\n",
    "\n",
    "While some truly illegible handwriting will remain nearly indecipherable by artificially intelligent interpreters, computers can quite easily distinguish individual letters and numbers. This classification is now commonplace in technologies such as Apple Watch's scribble-to-text.\n",
    "\n",
    "This exercise will carry out basic numeric classification for digits 0-9 using the MNIST dataset available in Python's ScikitLearn library. This exercise follows a tutorial in Packt's advanced machine learning course available at udemy.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "The data for this exercise consist of 70,000 images of handwritten digits represented as 784-digit vectors, derived from 28 X 28 pixel images. Each digit in those vectors can take values 0 to 1, 0 meaning no writing exists in that pixel in that image, and 1 meaning dark writing exists in that pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COL_NAMES': ['label', 'data'],\n",
       " 'DESCR': 'mldata.org dataset: mnist-original',\n",
       " 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'target': array([0., 0., 0., ..., 9., 9., 9.])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into separate objects\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52500, 784)\n",
      "(52500,)\n",
      "(17500, 784)\n",
      "(17500,)\n"
     ]
    }
   ],
   "source": [
    "# Partition data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    stratify = y,\n",
    "    shuffle = True,\n",
    "    random_state = 314\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Digit\n",
    "Below is an example of one of the digits in its original 28 x 28 pixel format. As explained previously, pixels without writing would appear as 0's, and the areas with the darkest writing would appear as 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10fdb20b8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fa818d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print example digit\n",
    "some_digit = X[25001]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image,cmap = matplotlib.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "In predicting digits, we will use two models, a random forest classifier and a support vector machine (SVM). \n",
    "\n",
    "The random forest will be fit using Scikit-Learn's default hyperparameters, with the exception of the number of trees, which will be adjusted from the default value of 10 to 100. The n_jobs argument allows the classifier to be fit using all available processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Fit random forest classifier\n",
    "rf = RandomForestClassifier(n_jobs = -1, n_estimators = 100, random_state = 314)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "print(\"OOB Accuracy: {:.2f}\".format(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bradensharp/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:258: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if np.all([l not in y_true for l in labels]):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least one label specified must be in y_true",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e53edf3ad5fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconfusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion matrix:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one label specified must be in y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: At least one label specified must be in y_true"
     ]
    }
   ],
   "source": [
    "# Measure performance\n",
    "pred_rf = rf.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, pred_rf, labels = labs)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an out of bag accuracy of 97%, the random forest looks promising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM will also initially be fit using Scikit-Learn's default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bradensharp/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 0.92\n",
      "Test set accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Fit scaled SVM\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "svm_model = svm.LinearSVC()\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "print(\"Train set accuracy: {:.2f}\".format(svm_model.score(X_train_scaled, y_train)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(svm_model.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[1659    1   15    1    2   15   15    4   12    2]\n",
      " [   0 1910   11    8    2   16    5    1   10    6]\n",
      " [  13   26 1544   26   12   10   31   34   37   15]\n",
      " [   9   12   43 1580    5   49   10   20   29   28]\n",
      " [   8    8    7    6 1574    3    7    4   20   69]\n",
      " [  10    8   20   60   13 1364   31    5   41   26]\n",
      " [  14   10   11    5   14   25 1626    7    5    2]\n",
      " [  10   10   23   12   25    4    0 1650   21   68]\n",
      " [  20   39   23   53   15   64   12   14 1441   25]\n",
      " [  11    4    9   40   64   13    0   52   21 1526]]\n"
     ]
    }
   ],
   "source": [
    "# Measure performance\n",
    "pred_svm = svm_model.predict(X_test_scaled)\n",
    "confusion = confusion_matrix(y_test, pred_svm)\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test-set accuracy for the SVM is still an impressive 91%, but it doesn't quite compete with an untuned random forest. In a classification task such as this, with ten balanced classes, accuracy alone is typically a good enough measure to determine the optimal model, but further investigation wouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      1726\n",
      "          1       0.99      0.99      0.99      1969\n",
      "          2       0.95      0.97      0.96      1748\n",
      "          3       0.96      0.95      0.96      1785\n",
      "          4       0.97      0.98      0.97      1706\n",
      "          5       0.97      0.97      0.97      1578\n",
      "          6       0.98      0.98      0.98      1719\n",
      "          7       0.97      0.96      0.97      1823\n",
      "          8       0.97      0.95      0.96      1706\n",
      "          9       0.95      0.95      0.95      1740\n",
      "\n",
      "avg / total       0.97      0.97      0.97     17500\n",
      "\n",
      "SVM:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.96      0.95      1726\n",
      "          1       0.94      0.97      0.96      1969\n",
      "          2       0.91      0.88      0.89      1748\n",
      "          3       0.88      0.89      0.88      1785\n",
      "          4       0.91      0.92      0.92      1706\n",
      "          5       0.87      0.86      0.87      1578\n",
      "          6       0.94      0.95      0.94      1719\n",
      "          7       0.92      0.91      0.91      1823\n",
      "          8       0.88      0.84      0.86      1706\n",
      "          9       0.86      0.88      0.87      1740\n",
      "\n",
      "avg / total       0.91      0.91      0.91     17500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports\n",
    "labs = [str(i) for i in list(range(10))]\n",
    "\n",
    "rf_class_report = classification_report(y_test, pred_rf, target_names = labs)\n",
    "svm_class_report = classification_report(y_test, pred_svm, target_names = labs)\n",
    "print('Random Forest:\\n{}'.format(rf_class_report))\n",
    "print('SVM:\\n{}'.format(svm_class_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Across all metrics, the random forest outperforms the SVM. Again, this was to be expected given the balanced classes and previously identified accuracy measures. While further tuning the hyperparameters of the random forest could increase that accuracy, many situational requirements would be satisfied with 97% accuracy.\n",
    "\n",
    "An additional piece of insight gleaned from this model is the relative difficulty machine learning models have predicting specific digits. Based on these data and these models, 0, 1, and 6 are the easiest numbers to predict, while 8 and 9 are more difficult."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
